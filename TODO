isaacsim isaacsim.exp.full.kit

WANDB_API_KEY=369fbf0326cfa2c16aab5babba5a478788d901f4 nohup python scripts/reinforcement_learning/rl_games/train_klask.py --task Isaac-Klask-v0 --num_envs 4096 --config planned_runs/sweep/sim_sparse_ro_a0_1.yaml --checkpoint logs/rl_games/klask/sim_distance_ro_a0_2/nn/klask.pth --headless --wandb-project-name Isaac-Klask --device cuda:1 --wandb-entity idsc-rda > output_1.log &
WANDB_API_KEY=369fbf0326cfa2c16aab5babba5a478788d901f4 python scripts/reinforcement_learning/rl_games/train_klask.py --task Isaac-Klask-v0 --num_envs 4096 --config planned_runs/sweep/sim_distance_sp_a0_2.yaml --checkpoint "logs/rl_games/klask/sim_collision_ro_a0_2/nn/klask.pth" --headless --wandb-project-name Isaac-Klask --wandb-entity idsc-rda


Implementation:
- possibly use curriculum manager instead of curriculum wrapper

Hyperparameters:
- (rl_games) network/separate: try True (seperate actor and critic networks; supposedly works better for continuous control)
- (rl_games) config/normalize_input: try True (normalize observations)
- (rl_games) env/clip_observations: check if current value (5.0) is good

Experiments:
- algorithms: PPO / SAC
- curricula: 
    - distance ball opponent goal -> + goal scored/conceded -> + time to goal scored
    - distance ball opponent goal + player in goal -> + goal scored/conceded -> + time to goal scored
    - collision player ball -> distance ball opponent goal -> + goal scored/conceded
- opponents: random / self-play current policy / self-play multiple checkpoints
- resets: only timeout / goal scored/conceded, player in goal, timeout

SimToReal:
- domain randomization: object dimensions, friction, restitution
- add noise to observations
- (action smoothness penalty)
- delay: include actions from last 200 ms in observations
- include collision avoidance in sim
- (model magnetic interaction)
- how to deal with differences in simulation  and real time step?

Performance:
- sbx: 22k fps
- sb3: 23k fps
- rl_games: 60k fps

Sweep:
- no sim-to-real (ideal PD actuator, no domain randomization):
    - collision -> distance to goal + sparse
    - MLP layers
    - PPO horizon
    - random opponent / self-play
    - action history / no action history
- sim-to-real:
    - domain randomization ranges
    - ideal PD actuator / delayed PD actuator
    - wall hit penalty
    - action history / no action history


ACTUATOR MODEL:
- dataloader:
    - inputs: history of commands (+ states) (+ delta_t); window [t-300ms, t] or [t-300ms, t-200ms]
    - outputs: peg velocity at time t
    - interpolation: randomly select points inbetween timestamps and linearly interpolate states (use nearest command)
